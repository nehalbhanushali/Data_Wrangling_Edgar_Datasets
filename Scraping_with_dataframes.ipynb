{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import namedtuple\n",
    "import urllib.request as ur # urllib2 is latest?\n",
    "from collections import namedtuple\n",
    "import pprint\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import zipfile\n",
    "#import tinys3\n",
    "\n",
    "class Page:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "\n",
    "        self.link = ur.urlopen(url)\n",
    "        \n",
    "    def get_hyperlink(self):\n",
    "        \n",
    "        raw_html = self.link.read()\n",
    "        soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "        \n",
    "\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile(\"10q.htm$\")}):\n",
    "            path = link.get('href')\n",
    "              \n",
    "            \n",
    "    def get_tables(self):\n",
    "        \"\"\"\n",
    "        Extracts each table on the page and places it in a dictionary.\n",
    "        Converts each dictionary to a Table object. Returns a list of\n",
    "        pointers to the respective Table object(s).\n",
    "        \"\"\"\n",
    "\n",
    "        raw_html = self.link.read()\n",
    "        soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "        # to get tables with background color only\n",
    "        tables = soup.findAll(\"table\",attrs={'border':1})\n",
    "        #tables = soup.findAll(\"table\")\n",
    "#         td=soup.find('td', attrs={'style': re.compile(\"^border:#B9CDE5\") })\n",
    "#         for mytr in td.parents:\n",
    "#             for mytable in mytr.parents:\n",
    "#                 tables.append(mytable)\n",
    "                \n",
    "        print(len(tables))\n",
    "        #tables = soup.findAll(\"table\") # actual resultset of tables to be used ultimately\n",
    "\n",
    "        ##### Test: one table\n",
    "        tables = soup.findAll(\"Nehal\")\n",
    "        # expl: is a tag, not resultset, cant loop over it to find tables,\n",
    "        # hence the errors in catching table 5. \n",
    "        \n",
    "        # Creating dummy list of tables with one table so that looping can be simulated\n",
    "        for i in range(0,106):\n",
    "            table = soup.findAll(\"table\",attrs={'border':1})[i]\n",
    "            tables.append(table)\n",
    "        print(len(tables)) \n",
    "            #print(\" type of table5 \",type(table))\n",
    "        \n",
    "        # print(\" type of tables1 now\",type(tables1))\n",
    "        \n",
    "        \n",
    "        ## Looping through all tables in soup fetched page remove _list vars if dataframe success\n",
    "        table_list = []\n",
    "        tableCount = 0\n",
    "        for table in tables:\n",
    "            dataTable = DataFrame()\n",
    "            df = dataTable.parse_html_table(table)\n",
    "            #print(\"Data Frame : \",df)\n",
    "            \n",
    "            tableRows = table.findAll(\"tr\")\n",
    "            rowCount = 0\n",
    "            for row in tableRows:\n",
    "                entries = row.findAll(\"td\")\n",
    "                tdCount = 0\n",
    "                for entry in entries:\n",
    "                    \n",
    "                    x = self.encode_text(entry) ## function not working\n",
    "                    \n",
    "\n",
    "                    #print(str(x).startswith(\"$\"),\" >>>> $\")    \n",
    "                   \n",
    "                    if( rowCount < 3 ):\n",
    "                        df.set_value(rowCount,tdCount,x)\n",
    "                    elif( rowCount >= 3):\n",
    "                        if(tdCount == 0):\n",
    "\n",
    "                            if(str(entry.text.strip())):\n",
    "                            #if re.compile('^[a-z0-9\\.]+$').match(str(x)):\n",
    "                                #print(x ,\" is string\")\n",
    "                                df.set_value(rowCount,tdCount,x) \n",
    "                            else:\n",
    "                                tdCount = tdCount -1\n",
    "                              \n",
    "                            #print(\" entry lengths \", rowCount)\n",
    "                        else:\n",
    "                            df.set_value(rowCount,tdCount,x)\n",
    "                            \n",
    "                    else :\n",
    "                        df.set_value(rowCount,tdCount,x)\n",
    "                       \n",
    "                    if str(x).find(\"$\") != -1: \n",
    "                        df.set_value(rowCount,tdCount,\"\") \n",
    "                        \n",
    "                    # looking for meaningful data\n",
    "                    #for i in entry.findAll('font',attrs={'style':'font-size:10.0pt;'}):\n",
    "                    tdCount+=1 \n",
    "                rowCount+=1\n",
    "            tableCount+=1\n",
    "            table_list.append(df)\n",
    "            #print(\"df \",type(df))\n",
    "            #df.to_csv('aktualnosci.csv',encoding='utf-8')\n",
    "            #df.to_csv('/EdgarFiles/Table_'+tableCount+'.csv')\n",
    "\n",
    "        return table_list\n",
    "    \n",
    "    def encode_text_not_working(self,x):\n",
    "        try:\n",
    "            x = x.text.encode(\"utf-8\")\n",
    "            strip_unicode = re.compile(\"([^-_a-zA-Z0-9!@#%&=,/'\\\";:~`\\$\\^\\*\\(\\)\\+\\[\\]\\.\\{\\}\\|\\?\\<\\>\\\\]+|[^\\s]+)\")\n",
    "            x = strip_unicode.sub(\"\", x.decode(\"utf-8\"))\n",
    "            \n",
    "            \n",
    "        except Exception:\n",
    "            #print 'encoding error: {0} {1}'.format(rowCount, tdCount)\n",
    "            x = \"\"\n",
    "            \n",
    "        return x  \n",
    "    \n",
    "    def encode_text(self,x):\n",
    "        try:\n",
    "            x = x.text.strip().encode(\"utf-8\")\n",
    "            \n",
    "        except Exception:\n",
    "            #print 'encoding error: {0} {1}'.format(rowCount, tdCount)\n",
    "            x = \"\"\n",
    "            \n",
    "        return x \n",
    "\n",
    "    def save_tables(self, tables, ignore_small=False):\n",
    "        \"\"\"\n",
    "        Takes an input a list of table objects and saves each\n",
    "        table to csv format. If ignore_small is True,\n",
    "        we ignore any tables with 5 entries or fewer. \n",
    "        \"\"\"\n",
    "        \n",
    "        counter = 1\n",
    "        for table in tables:\n",
    "            if ignore_small:\n",
    "                if table.get_metadata().num_entries > 5:\n",
    "                    name = \"EdgarFiles/table_\" + str(counter) +\".csv\"\n",
    "                    table.to_csv(name)\n",
    "                    counter += 1\n",
    "            else:\n",
    "                name = \"EdgarFiles/Table_5_\" + str(counter)+\".csv\" ## todo remove trialcsv\n",
    "                table.to_csv(name)\n",
    "                counter += 1\n",
    "\n",
    "\n",
    "    def create_directory(self,path):\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "        except OSError as exception:\n",
    "            if exception.errno != errno.EEXIST:\n",
    "                raise\n",
    "    \n",
    "    def create_zip_folder(self,path):\n",
    "        zipfolder_name=path+'.zip'\n",
    "        zf = zipfile.ZipFile(zipfolder_name, \"w\")\n",
    "        for dirname, subdirs, files in os.walk(path):\n",
    "            zf.write(dirname)\n",
    "            for filename in files:\n",
    "                zf.write(os.path.join(dirname, filename))\n",
    "        zf.close()\n",
    "    \n",
    "    def upload_zip_to_s3(self,path):\n",
    "        S3_ACCESS_KEY = ConfigSectionMap(\"Part_1\")['s3_access_key']#'AKIAICSMTFLAR54DYMQQ'\n",
    "        S3_SECRET_KEY = ConfigSectionMap(\"Part_1\")['s3_secret_key']#'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'\n",
    "        host='edgardatasets.s3-website-us-west-2.amazonaws.com'\n",
    "        # Creating a simple connection\n",
    "        conn = tinys3.Connection(S3_ACCESS_KEY,S3_SECRET_KEY)\n",
    "\n",
    "        # Uploading a single file\n",
    "        f = open('EdgarFiles.zip','rb')\n",
    "        conn.upload('EdgarFiles.zip',f,path)\n",
    "                \n",
    "                \n",
    "Metadata = namedtuple(\"Metadata\", \"num_cols num_entries\")\n",
    "\n",
    "\n",
    "class DataFrame:\n",
    "    \n",
    "     def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor ??? ## todo\n",
    "        \"\"\"\n",
    "\n",
    "       \n",
    "    \n",
    "     def parse_html_table(self,table):\n",
    "            n_columns = 0\n",
    "            n_rows=0\n",
    "            column_names = []\n",
    "    \n",
    "            # Find number of rows and columns\n",
    "            # we also find the column titles if we can\n",
    "            for row in table.find_all('tr'):\n",
    "                \n",
    "                # Determine the number of rows in the table\n",
    "                td_tags = row.find_all('td')\n",
    "                if len(td_tags) > 0:\n",
    "                    n_rows+=1\n",
    "                    if n_columns == 0 or n_columns < len(td_tags):\n",
    "                        # Set the number of columns for our table\n",
    "                        n_columns = len(td_tags)\n",
    "                        \n",
    "                # Test : Handle column names if we find them\n",
    "                th_tags = row.find_all('th') \n",
    "                if len(th_tags) > 0 and len(column_names) == 0:\n",
    "                    for th in th_tags:\n",
    "                        column_names.append(th.get_text())\n",
    "            \n",
    "            #print(\"cols/rows frame 1 >>>> \",n_columns,\" \",n_rows)\n",
    "            \n",
    "            # Safeguard on Column Titles\n",
    "            if len(column_names) > 0 and len(column_names) != n_columns:\n",
    "                raise Exception(\"Column titles do not match the number of columns\")\n",
    "    \n",
    "            columns = column_names if len(column_names) > 0 else range(0,n_columns)\n",
    "            df = pd.DataFrame(columns = columns,\n",
    "                              index= range(0,n_rows))\n",
    "            \n",
    "            #print(\"data frame first time >>>> \"+df)\n",
    "            \n",
    "            \"\"\"\"\n",
    "            row_marker = 0\n",
    "            for row in table.find_all('tr'):\n",
    "                column_marker = 0\n",
    "                columns = row.find_all('td')\n",
    "                for column in columns:\n",
    "                    df.iat[row_marker,column_marker] = column.get_text()\n",
    "                    column_marker += 1\n",
    "                if len(columns) > 0:\n",
    "                    row_marker += 1\n",
    "                    \n",
    "            # Convert to float if possible\n",
    "            for col in df:\n",
    "                try:\n",
    "                    df[col] = df[col].astype(float)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            print(\"weird frame >>>> \"+df)\n",
    "            \"\"\"\n",
    "            return df\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "############ Do we need main here >>??? ###\n",
    "\n",
    "# mainUrl = \"http://www.sec.gov/Archives/edgar/data/\"\n",
    "\n",
    "# #CIK = input(\"Enter CIK (eg :0000051143)\")\n",
    "# #DAN = input(\"Enter document accession number -DAN (eg :000005114313000007 )\")\n",
    "# CIK=ConfigSectionMap(\"Part_1\")['cik']\n",
    "# DAN=ConfigSectionMap(\"Part_1\")['dan']\n",
    "\n",
    "# CIK = CIK.strip().strip(\"0\")\n",
    "# DAN = DAN.strip()\n",
    "# partCIK = DAN[0:10]+\"-\"\n",
    "# partDAN = DAN[10:12]+\"-\"\n",
    "# lastPart = DAN[12:18]+\"-index.html\"\n",
    "\n",
    "# completeURL = mainUrl+CIK+\"/\"+DAN+\"/\"+partCIK+partDAN+lastPart\n",
    "# #print(completeURL)\n",
    "# html = Page(completeURL)\n",
    "# link1 = html.get_hyperlink()\n",
    "\n",
    "\n",
    "# url that contains the tables we want\n",
    "url=\"https://www.sec.gov/Archives/edgar/data/51143/000005114313000007/ibm13q3_10q.htm\"\n",
    "\n",
    "#print(\" >>>>>>> match \"+completeURL == url)\n",
    "logger = logging.getLogger()\n",
    "page = Page(url)\n",
    "tables = page.get_tables()\n",
    "page.create_directory(\"EdgarFiles\")\n",
    "page.save_tables(tables, ignore_small=False)\n",
    "page.create_zip_folder('EdgarFiles')\n",
    "#page.upload_zip_to_s3('edgardatasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "Config = configparser.ConfigParser()\n",
    "Config.read('config.ini')\n",
    "Config.sections()\n",
    "def ConfigSectionMap(section):\n",
    "    dict1 = {}\n",
    "    options = Config.options(section)\n",
    "    for option in options:\n",
    "        try:\n",
    "            dict1[option] = Config.get(section, option)\n",
    "            if dict1[option] == -1:\n",
    "                DebugPrint(\"skip: %s\" % option)\n",
    "        except:\n",
    "            print(\"exception on %s!\" % option)\n",
    "            dict1[option] = None\n",
    "    return dict1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ConfigSectionMap(\"Part_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
