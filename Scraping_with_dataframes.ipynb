{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter CIK (eg :0000051143)0000051143\n",
      "Enter document accession number -DAN (eg :000005114313000007 )000005114313000007 \n",
      "Scraping the webpage to find 10k/q html file: http://www.sec.gov/Archives/edgar/data/51143/000005114313000007/0000051143-13-000007-index.html\n",
      "10k/q file : https://www.sec.gov/Archives/edgar/data/51143/000005114313000007/ibm13q3_10q.htm\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import namedtuple\n",
    "import urllib.request as ur # urllib2 is latest?\n",
    "from collections import namedtuple\n",
    "import pprint\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import zipfile\n",
    "import sys\n",
    "import tinys3\n",
    "\n",
    "class Page:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.link = ur.urlopen(url)\n",
    "            self.fileName=\"EdgarFiles\"\n",
    "        except Exception:\n",
    "            print(\"INVALID URL. Please check CIK or Accession number\")\n",
    "            sys.exit()\n",
    "        \n",
    "    def get_hyperlink(self):\n",
    "        \n",
    "        raw_html = self.link.read()\n",
    "        soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "        path = 'https://www.sec.gov'\n",
    "\n",
    "        for row in soup.findAll('tr')[1:2]:\n",
    "            \n",
    "            for td in row.findAll('a'):\n",
    "                if \"10\" in td.text:\n",
    "#                     print(\"gsHA \",td.get('href'))\n",
    "                    path += td.get('href')\n",
    "                   \n",
    "  \n",
    "        return path     \n",
    "              \n",
    "            \n",
    "    def get_tables(self):\n",
    "        \"\"\"\n",
    "        Extracts each table on the page and places it in a dictionary.\n",
    "        Converts each dictionary to a Table object. Returns a list of\n",
    "        pointers to the respective Table object(s).\n",
    "        \"\"\"\n",
    "\n",
    "        raw_html = self.link.read()\n",
    "        soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "        tables = soup.findAll(\"table\")\n",
    "\n",
    "        actual =  len(tables)       \n",
    "        print(\"Actual number of tables\",len(tables))\n",
    "\n",
    "        ##### Test: one table\n",
    "        tables = soup.findAll(\"Nehal\")\n",
    "        \n",
    "        # Creating dummy list of tables with one table so that looping can be simulated\n",
    "        for i in range(0,actual):\n",
    "            table = soup.findAll(\"table\")[i] # todo align with alt row color changes later\n",
    "            tables.append(table)\n",
    "            \n",
    "        print(\"Looping through \",len(tables), \" tables\") \n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Looping through all tables in soup fetched page \n",
    "        table_list = []\n",
    "        tableCount = 0\n",
    "        for table in tables:\n",
    "            coloredTD = table.findAll('td', attrs={'style':re.compile(r'background')})\n",
    "#             print(\"no of colord tds \",len(coloredTD))\n",
    "            if(len(coloredTD) >0):\n",
    "                dataTable = DataFrame()\n",
    "                df = dataTable.parse_html_table(table)\n",
    "                #print(\"Data Frame : \",df)\n",
    "                self.fileName = \"\"\n",
    "                tableRows = table.findAll(\"tr\")\n",
    "                rowCount = 0\n",
    "                for row in tableRows:\n",
    "                    entries = row.findAll(\"td\")\n",
    "                    tdCount = 0\n",
    "                    for entry in entries:\n",
    "\n",
    "                        x = self.encode_text(entry) ## function not working\n",
    "\n",
    "\n",
    "                        #print(str(x).startswith(\"$\"),\" >>>> $\")    \n",
    "\n",
    "                        if( rowCount < 3 ):\n",
    "                            df.set_value(rowCount,tdCount,x)\n",
    "                        elif( rowCount >= 3):\n",
    "\n",
    "                            if(tdCount == 0):\n",
    "\n",
    "                                if(str(entry.text.strip())):\n",
    "                                #if re.compile('^[a-z0-9\\.]+$').match(str(x)):\n",
    "                                    #print(x ,\" is string\")\n",
    "                                    df.set_value(rowCount,tdCount,x) \n",
    "                                else:\n",
    "                                    tdCount = tdCount -1\n",
    "\n",
    "                                #print(\" entry lengths \", rowCount)\n",
    "                            else:\n",
    "                                df.set_value(rowCount,tdCount,x)\n",
    "\n",
    "                        else :\n",
    "                            df.set_value(rowCount,tdCount,x)\n",
    "\n",
    "                        if str(x).find(\"$\") != -1: \n",
    "                            df.set_value(rowCount,tdCount,\"\") \n",
    "\n",
    "                        tdCount+=1 \n",
    "                    rowCount+=1\n",
    "                tableCount+=1\n",
    "                table_list.append(df)\n",
    "        print(\"Tables with sensible data : \",tableCount)    \n",
    "        return table_list\n",
    "    \n",
    "    def encode_text(self,x):\n",
    "        try:\n",
    "            x = x.text.strip().encode(\"utf-8\")\n",
    "            x = x.decode(\"utf-8\")\n",
    "            special_chars = [\"[!@#$]\", \"%\",\":\"]\n",
    "            x=self.string_cleanup(x,special_chars)\n",
    "        except Exception:\n",
    "            #print 'encoding error: {0} {1}'.format(rowCount, tdCount)\n",
    "            x = \"\"\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def string_cleanup(self,x, notwanted):\n",
    "        for item in notwanted:\n",
    "            x = re.sub(item, '', x)\n",
    "        return x\n",
    "\n",
    "    def save_tables(self, tables, ignore_small=False):\n",
    "        \"\"\"\n",
    "        Takes an input a list of table objects and saves each\n",
    "        table to csv format. If ignore_small is True,\n",
    "        we ignore any tables with 5 entries or fewer. \n",
    "        \"\"\"\n",
    "        \n",
    "        counter = 1\n",
    "        for table in tables:\n",
    "           \n",
    "            \n",
    "            if ignore_small:\n",
    "                if table.get_metadata().num_entries > 5:\n",
    "                    #print(\" big > 5 \",table.get_value(3, 0),\" and \",table.get_value(0, 4))\n",
    "                    x = table.get_value(0, 0)       \n",
    "                    self.fileName=x.replace(\"\\n\", \"_and_\").replace(\" \",\"_\").replace(\"/\",\"_or_\")\n",
    "                    name = \"EdgarFiles/\"+CIK+\"/\"+ str(counter)+\"_\" +self.fileName+\".csv\"\n",
    "                    try:\n",
    "                        table.to_csv(name)\n",
    "                        print(\"printing if it gets table\"+table.to_csv(name))\n",
    "                    except Exception:\n",
    "                        \n",
    "                        print(\"exception in creating files is\",Exception)\n",
    "                        print(\"exception in creating files\"+table.to_csv(name))\n",
    "                    counter += 1\n",
    "            else:\n",
    "                try:\n",
    "                    x = table.get_value(3, 0) ## to name the file with first meaningful variable found at 0,3      \n",
    "                except Exception:                        \n",
    "                    x = table.get_value(0, 0)  ## for small files with no 3rd row\n",
    "\n",
    "                self.fileName=str(x).replace(\"\\n\", \"_and_\").replace(\" \",\"_\").replace(\"/\",\"_or_\").replace(\"*\",\"\")\n",
    "\n",
    "                name = \"EdgarFiles/\"+CIK+\"/\" + str(counter)+\"_\"+self.fileName +\".csv\" ## todo remove trialcsv\n",
    "                table.to_csv(name)\n",
    "                #print(os.getcwd())\n",
    "                counter += 1\n",
    "\n",
    "\n",
    "    def create_directory(self,path):\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "        except OSError as exception:\n",
    "            if exception.errno != errno.EEXIST:\n",
    "                raise\n",
    "    \n",
    "    def create_zip_folder(self,path):\n",
    "        print(\"Creating Zip folder\")\n",
    "        zipfolder_name=path+'.zip'\n",
    "        zf = zipfile.ZipFile(zipfolder_name, \"w\")\n",
    "        for dirname, subdirs, files in os.walk(path):\n",
    "            zf.write(dirname)\n",
    "            for filename in files:\n",
    "                zf.write(os.path.join(dirname, filename))\n",
    "        zf.close()\n",
    "    \n",
    "    def upload_zip_to_s3(self,filetoupload):\n",
    "        print(\"Upload to s3\")\n",
    "        S3_ACCESS_KEY= input(\"Enter S3_ACCESS_KEY : \")\n",
    "        S3_SECRET_KEY =  input(\"Enter S3_SECRET_KEY : \")\n",
    "        \n",
    "        try:\n",
    "            bucket = input(\"Enter BUCKET_NAME : \")\n",
    "            my_endpoint = \"s3-us-west-1.amazonaws.com\"\n",
    "            conn = tinys3.Connection(S3_ACCESS_KEY,S3_SECRET_KEY,tls=True,endpoint=my_endpoint)\n",
    "            f = open(filetoupload,'rb')\n",
    "            conn.upload(filetoupload,f,bucket)\n",
    "            print(\"Upload to s3 successfull\")\n",
    "           \n",
    "        except Exception:\n",
    "            print(\"INVALID keys, please try again\")\n",
    "            self.upload_zip_to_s3(filetoupload)\n",
    "    \n",
    "Metadata = namedtuple(\"Metadata\", \"num_cols num_entries\")\n",
    "\n",
    "\n",
    "class DataFrame:\n",
    "    \n",
    "     def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor ??? ## todo\n",
    "        \"\"\"\n",
    "\n",
    "       \n",
    "    \n",
    "     def parse_html_table(self,table):\n",
    "            n_columns = 0\n",
    "            n_rows=0\n",
    "            column_names = []\n",
    "\n",
    "            # Find number of rows and columns\n",
    "            # we also find the column titles if we can\n",
    "            for row in table.find_all('tr'):\n",
    "                \n",
    "                # Determine the number of rows in the table\n",
    "                td_tags = row.find_all('td')\n",
    " \n",
    "                if len(td_tags) > 0:\n",
    "                    n_rows+=1\n",
    "                    if n_columns == 0 or n_columns < len(td_tags):\n",
    "                        # Set the number of columns for our table\n",
    "                        n_columns = len(td_tags)\n",
    "                        \n",
    "                # Test : Handle column names if we find them\n",
    "                th_tags = row.find_all('th') \n",
    "                if len(th_tags) > 0 and len(column_names) == 0:\n",
    "                    for th in th_tags:\n",
    "                        column_names.append(th.get_text())\n",
    "            \n",
    "            #print(\"cols/rows frame 1 >>>> \",n_columns,\" \",n_rows)\n",
    "            \n",
    "            # Safeguard on Column Titles\n",
    "            if len(column_names) > 0 and len(column_names) != n_columns:\n",
    "                raise Exception(\"Column titles do not match the number of columns\")\n",
    "    \n",
    "            columns = column_names if len(column_names) > 0 else range(0,n_columns)\n",
    "            df = pd.DataFrame(columns = columns,\n",
    "                              index= range(0,n_rows))\n",
    "            \n",
    "            #print(\"data frame first time >>>> \"+df)\n",
    "            \n",
    "            \"\"\"\"\n",
    "            row_marker = 0\n",
    "            for row in table.find_all('tr'):\n",
    "                column_marker = 0\n",
    "                columns = row.find_all('td')\n",
    "                for column in columns:\n",
    "                    df.iat[row_marker,column_marker] = column.get_text()\n",
    "                    column_marker += 1\n",
    "                if len(columns) > 0:\n",
    "                    row_marker += 1\n",
    "                    \n",
    "            # Convert to float if possible\n",
    "            for col in df:\n",
    "                try:\n",
    "                    df[col] = df[col].astype(float)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            print(\"weird frame >>>> \"+df)\n",
    "            \"\"\"\n",
    "            return df\n",
    "\n",
    "\n",
    "mainUrl = \"http://www.sec.gov/Archives/edgar/data/\"\n",
    "\n",
    "CIK = input(\"Enter CIK (eg :0000051143)\")\n",
    "DAN = input(\"Enter document accession number -DAN (eg :000005114313000007 )\")\n",
    "\n",
    "CIK = CIK.strip().strip(\"0\")\n",
    "DAN = DAN.strip()\n",
    "partCIK = DAN[0:10]+\"-\"\n",
    "partDAN = DAN[10:12]+\"-\"\n",
    "lastPart = DAN[12:18]+\"-index.html\"\n",
    "\n",
    "completeURL = mainUrl+CIK+\"/\"+DAN+\"/\"+partCIK+partDAN+lastPart\n",
    "print(\"Scraping the webpage to find 10k/q html file: \"+completeURL)\n",
    "html = Page(completeURL)\n",
    "link1 = html.get_hyperlink()\n",
    "print(\"10k/q file : \"+link1)\n",
    "\n",
    "### sample url that contains the tables we want\n",
    "#url=\"https://www.sec.gov/Archives/edgar/data/51143/000005114313000007/ibm13q3_10q.htm\"\n",
    "#url=\"https://www.sec.gov/Archives/edgar/data/1652044/000165204417000008/goog10-kq42016.htm\"\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "page = Page(link1)\n",
    "\n",
    "tables = page.get_tables()\n",
    "\n",
    "page.create_directory(\"EdgarFiles/\"+CIK)\n",
    "\n",
    "page.save_tables(tables, ignore_small=False)\n",
    "\n",
    "page.create_zip_folder('EdgarFiles')\n",
    "\n",
    "page.upload_zip_to_s3('EdgarFiles.zip')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
